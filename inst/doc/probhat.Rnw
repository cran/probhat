%\VignetteIndexEntry{Multivariate Generalized Kernel Smoothing and Related Statistical Methods}
\documentclass{article}
\usepackage[a4paper,top=2.6cm,bottom=3.6cm,left=3.6cm,right=3.6cm]{geometry}
\usepackage{parskip,verbatim,amsmath,amssymb,color}
\usepackage[nogin]{Sweave}
\pagestyle{myheadings}
\setlength{\parskip}{0.28cm}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{xleftmargin=0.75em, formatcom=\color{rin}}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=0.75em,formatcom=\color{rout}}
\DefineVerbatimEnvironment{Serror}{Verbatim}{xleftmargin=0.75em,formatcom=\color{rerr}}
\newcommand {\stitle}[3]
{	\title {\vspace {-0.6cm} {\normalsize #1 #2 } \\[0.8cm] {\textbf {\huge #3} } }
	\author {\textbf {Abby Spurdle} }
	\maketitle
	\markright{Spurdle, A.\hfill #1 #2\hfill}
	\thispagestyle {empty}
}
\newcommand {\sabstract}[1]
{	\begin {center}
	\begin {minipage}{14.25cm}
		{\textsl {#1} }
	\end {minipage}
	\end {center}
	\vspace {0.06cm}
}
\definecolor{db}{rgb}{0.1, 0, 0.55}
\definecolor{rin}{rgb}{0, 0, 0.32}
\definecolor{rout}{rgb}{0, 0.14, 0}
\definecolor{rerr}{rgb}{0.5, 0.025, 0}
\SweaveOpts{keep.source=TRUE}
\SweaveOpts{eps=FALSE}
\SweaveOpts{prefix.string=temp-probhat}
\begin{document}

<<echo=false>>=
options(continue=" ")
options(SweaveHooks=list(fig=function()
par(mar=c(4.1, 4.1, 2.6, 1.6), cex=0.7, cex.main=1)))
set.seed (1)
@

\newcommand{\pnt}{$\bullet$}
\newcommand{\pntsq}{\tiny $\blacksquare$}
\newcommand{\pntst}{$\circ$}
\newcommand{\tmu}[1]{\textbf {\textsf {\color{db} #1}}}
\newcommand{\ind}{\hspace {0.375cm} }
\newcommand{\indf}{\vspace {-0.175cm} \hspace {0.375cm} }

\newcommand{\dks}{$\sim$DKS}
\newcommand{\cks}{$\sim$CKS}
\newcommand{\cat}{$\sim$CAT}
\newcommand{\el}{$\sim$EL}

\newcommand{\pmfuvdks}{PMF\textsubscript{(UV)}$\sim$DKS}
\newcommand{\cdfuvdks}{CDF\textsubscript{(UV)}$\sim$DKS}
\newcommand{\qfuvdks}{QF\textsubscript{(UV)}$\sim$DKS}

\newcommand{\pdfuvcks}{PDF\textsubscript{(UV)}$\sim$CKS}
\newcommand{\cdfuvcks}{CDF\textsubscript{(UV)}$\sim$CKS}
\newcommand{\qfuvcks}{QF\textsubscript{(UV)}$\sim$CKS}
\newcommand{\pdfmvcks}{PDF\textsubscript{(MV)}$\sim$CKS}
\newcommand{\cdfmvcks}{CDF\textsubscript{(MV)}$\sim$CKS}
\newcommand{\pdfccks}{PDF\textsubscript{(C)}$\sim$CKS}
\newcommand{\cdfccks}{CDF\textsubscript{(C)}$\sim$CKS}
\newcommand{\qfccks}{QF\textsubscript{(C)}$\sim$CKS}
\newcommand{\pdfmvccks}{PDF\textsubscript{(MVC)}$\sim$CKS}
\newcommand{\cdfmvccks}{CDF\textsubscript{(MVC)}$\sim$CKS}
\newcommand{\chqfcks}{ChQF$\sim$CKS}

\newcommand{\pmfuvcat}{PMF\textsubscript{(UV)}$\sim$CAT}
\newcommand{\cdfuvcat}{CDF\textsubscript{(UV)}$\sim$CAT}
\newcommand{\qfuvcat}{QF\textsubscript{(UV)}$\sim$CAT}
\newcommand{\pmfccatcks}{PMF\textsubscript{(C)}$\sim$CAT|CKS}

\newcommand{\cdfel}{CDF$\sim$EL}
\newcommand{\qfel}{QF$\sim$EL}

\stitle {probhat}{0.2.0}{Multivariate Generalized\\Kernel Smoothing\\and\\Related Statistical Methods}

\sabstract {Constructs, plots and evaluates probability distributions (probability mass/density functions, cumulative distribution functions and quantile functions) with continuous kernel smoothing, and to a lesser extent, discrete kernel smoothing. Supports univariate, multivariate and conditional distributions, including multivariate-conditional distributions. Also, supports other probability distributions (categorical, frequency and empirical-like) and weighted data, which is possibly useful mixed with fuzzy clustering. Furthermore, there are extensions for computing multivariate probabilities and multivariate random numbers, and for parameter and mode estimation.}

\begin {center} \tmu {***note that this package is subject to change***} \end {center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Pre-Intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This package is based on self-referencing function objects.

Some functions return objects (here, mostly probability distributions), which are also functions.

The resulting function objects have attributes, which are accessible inside the function body.\\
(This enables functions to be bundled with data).

In this context, it's equivalent to the \{d, p, q, r\} approach used in R's stats package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This is an R package for multivariate generalized kernel smoothing, as per the title.

Kernel smoothing is generalized, by estimating:
\begin{itemize}
\itemsep -0.15cm
	\item Both discrete and continuous probability distributions.
	\item Probability mass/density functions (PMFs/PDFs), cumulative distribution functions (CDFs) and quantile functions (QFs).
	\item In the continuous case, multivariate, conditional and weighted distributions.
\end{itemize}

Also, there are categorical and empirical-like distributions, both of which, may be weighted.

Specifically, this includes:
\begin{itemize}
	\item With discrete kernel smoothing (\dks):\\
	(Primarily, for modelling smoothed-frequency distributions).
	\begin{itemize}
	\itemsep -0.15cm
		\item [\pnt] Univariate probability mass function (\pmfuvdks).
		\item [\pnt] Univariate cumulative distribution function (\cdfuvdks).
		\item [\pnt] Univariate quantile function (\qfuvdks).
	\end{itemize}
	\item With continuous kernel smoothing (\cks):
	\begin{itemize}
	\itemsep -0.15cm
		\item [\pnt] Univariate probability density function (\pdfuvcks).
		\item [\pnt] Univariate cumulative distribution function (\cdfuvcks).
		\item [\pnt] Univariate quantile function (\qfuvcks).
		\item [\pntsq] Multivariate probability density function (\pdfmvcks).
		\item [\pntsq] Multivariate cumulative distribution function (\cdfmvcks).
		\item [\pnt] Conditional probability density function (\pdfccks).
		\item [\pnt] Conditional cumulative distribution function (\cdfccks).
		\item [\pnt] Conditional quantile function (\qfccks).
		\item [\pntsq] Multivariate-conditional probability density function (\pdfmvccks).
		\item [\pntsq] Multivariate-conditional cumulative distribution function (\cdfmvccks).
		\item [\pntst] Chained quantile function (\chqfcks).
	\end{itemize}
	\item Categorical distributions (\cat):
	\begin{itemize}
	\itemsep -0.15cm
		\item [\pnt] Univariate probability mass function (\pmfuvcat).
		\item [\pnt] Univariate cumulative distribution function (\cdfuvcat).
		\item [\pnt] Univariate quantile function (\qfuvcat).
		\item [\pnt] Conditional probability mass function (\pmfccatcks), conditional on a continuous variable.
	\end{itemize}
	\item Empirical-like distributions (\el):
	\begin{itemize}
	\itemsep -0.15cm
		\item [\pnt] (Univariate) cumulative distribution function (\cdfel).
		\item [\pnt] (Univariate) quantile function (\qfel).
	\end{itemize}
	\item Distribution sets:
	\begin{itemize}
	\itemsep -0.15cm
		\item [\pnt] Marginal sets.
		\item [\pnt] Categorical sets.
		\item [\pnt] Conditional sets.
	\end{itemize}
\end{itemize}

Here, univariate and multivariate models refer to unconditional distributions, unless stated otherwise. Conditional models refers to univariate-conditional distributions, unless stated otherwise. And multivariate-conditional models refer to the special case where a probability distribution is both multivariate and conditional.

By default, \dks\ models are lower-bounded and have a bandwidth parameter of one, which results in an (unsmoothed) frequency distribution. By default, univariate and conditional PDF/CDF \cks\ models use a cubic Hermite spline as an intermediate model.

Categorical variables are assumed to be ordinal, however, this assumption is only relevant for a meaningful interpretation of the CDF and QF. Empirical-like models are derived from empirical cumulative distribution functions. There's a small modification to the (initial) formula, and the resulting points are interpolated by a cubic Hermite spline, in a similar way to \cks\ models.

Also, {\cks}/{\cat}/{\el} models can be weighted, and I've provided an example of modelling a fuzzy cluster with weighted multivariate kernel density estimation, in an appendix, at the end of this vignette.

There are plot methods for all univariate models and distribution sets, and for multivariate models but only with two random variables.

Often the goal of kernel smoothing is simply to plot the distribution, as an exploratory tool.

However, these models can be used for a variety of purposes, including:
\begin{itemize}
\itemsep -0.15cm
    \item Computation of probabilities, from the CDF.
    \item Random number generation, from QFs.
    \item Computation of the mean, standard deviation, variance, skewness and kurtosis, from the PMF or continuous CDF.
    \item Computation of the median or quantiles, from QFs.
    \item Mode estimation, using the PMF or PDF.
\end{itemize}

Noting that all of the above apply to conditional distributions, too.

In principle, there's no multivariate quantile function, however, I've created a (novel) chained quantile function, to support the computation of multivariate random numbers.\\
(i.e. Synthetic data).

Note that currently, this package doesn't support automatic bandwidth selection.

This feature is likely to be added in the next update.

One solution, is to plot marginal PMFs/PDFs, and select the smallest bandwidth that doesn't look like it causing overfitting.

Also note that most of the models in this package work best with matrix objects (not data.frame objects), which have column names.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Preliminary Code}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
I'm going to load (and attach) the probhat, fclust and scatterplot3d packages:
<<>>=
library (probhat)
library (fclust)
library (scatterplot3d)
@

Note that the probhat package imports the intoo, barsurf and kubik packages.

I will set the theme for default colors to green:

<<>>=
use.ph.theme ("green")
@

And I will construct some data objects:

<<>>=
data.prep ()
@

This function emulates a script, the contents of which, are given later, in an appendix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Discrete Kernel Smoothing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can construct a \pmfuvdks\ object, using the \tmu{pmfuv.dks} constructor.

I will use traffic data, derived from the ``Traffic'' data in the MASS package:

<<>>=
fh = pmfuv.dks (traffic.x, traffic.h, bw=23, lower=0)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (fh, TRUE)
@
\end {center}

The ``x'' variable is number of accidents, and the ``y'' variable is the frequency.

Refer to the help file, for how to construct these objects, with other combinations of the first two arguments.

The resulting object is a function, which maps an integer vector (of integer quantiles) to a numeric vector (of masses):

<<>>=
fh (10)
@

Likewise, we can construct \cdfuvdks\ and \qfuvdks\ objects, using the \tmu{cdfuv.dks} and \tmu{qfuv.dks} constructors:

<<>>=
Fh = cdfuv.dks (traffic.x, traffic.h, bw=23, lower=0)
Fh.inv = qfuv.dks (traffic.x, traffic.h, bw=23, lower=0)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (Fh, TRUE)
@

<<fig=TRUE, width=4.75, height=3>>=
plot (Fh.inv)
@
\end {center}

Discrete quantile functions, are defined in the same way as R's stats package, and map a numeric vector (in the interval [0, 1]) to an integer vector.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Continuous Kernel Smoothing:\\Univariate Probability Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By default, univariate (and conditional) PDF/CDF \cks\ models use a cubic Hermite spline as an intermediate model, which is more efficient, if the probability distribution needs to be evaluated many times.

\qfuvcks\ (and \qfccks) models use either a cubic Hermite spline or a nested spline.

In principle, \qfuvcks\ models are constructed by transposing the CDF, however, if there are level sections in the CDF (i.e. zero-density regions in the corresponding PDF), then a nested spline is constructed, with a separate cubic Hermite spline for each increasing section of the CDF.

We can construct a \pdfuvcks\ object, using the \tmu{pdfuv.cks} constructor.

I will use the height variable derived from the ``trees'' data in the datasets package:

<<>>=
fh = pdfuv.cks (Height)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (fh, TRUE)
@
\end {center}

Note that this data has been converted to metric.

The resulting object is a function, which maps a numeric vector (of one or more quantiles) to a numeric vector (of one or more densities):

<<>>=
fh (22)
@

Likewise, we can construct \cdfuvcks\ and \qfuvcks\ objects, using the \tmu{cdfuv.cks} and \tmu{qfuv.cks} constructors:

<<>>=
Fh = cdfuv.cks (Height)
Fh.inv = qfuv.cks (Height)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (Fh, TRUE)
@

<<fig=TRUE, width=4.75, height=3>>=
plot (Fh.inv)
@
\end {center}

Continuous quantile functions, map a numeric vector (in the interval [0, 1]) to a numeric vector.

Note that here, they're not the exact inverse of the CDF, however, they become closer to the CDF, if the number of control points in the spline is increased.

<<>>=
p1 = 0.5
p2 = Fh (Fh.inv (p1) )
@

<<>>=
p1 == p2
abs (p1 - p2)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Continuous Kernel Smoothing:\\Multivariate Probability Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can construct a \pdfmvcks\ object, using the \tmu{pdfmv.cks} constructor.

Again, I will use the ``trees'' data:

<<>>=
fh = pdfmv.cks (trees [,2:3])
@
\begin {center}
<<fig=TRUE, width=4.75, height=4>>=
plot (fh, all=TRUE)
@
\end {center}

The resulting object is a function, which maps a numeric vector (implying a single row matrix) or matrix to a numeric vector:

<<>>=
fh (c (22, 0.8) )
@

Likewise, we can construct a \cdfmvcks\ object, using the \tmu{cdfuv.cks} constructor:
<<>>=
Fh = cdfmv.cks (trees [,2:3])
@

\begin {center}
<<fig=TRUE, width=4.75, height=4>>=
plot (Fh, all=TRUE)
@
\end {center}

Also, it's possible to compute what I refer to as chained quantile functions, discussed later.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Continuous Kernel Smoothing:\\Conditional Probability Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can construct a \pdfccks\ object, using the \tmu{pdfc.cks} constructor:

<<>>=
conditions = c (Girth=30, Height=22)
cfh = pdfc.cks (trees, conditions=conditions)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (cfh)
@
\end {center}

As with previous objects, it's a function which can be evaluated:
<<>>=
#density of volume (volume=0.85), given girth=30 and height=22
cfh (0.85)
@

Likewise, we can construct \cdfccks\ and \qfccks\ objects, using the \tmu{cdfc.cks} and \tmu{qfc.cks} constructors.

The resulting functions are almost identical to univariate functions, so I will bypass the examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Continuous Kernel Smoothing:\\Multivariate-Conditional Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can construct a \pdfmvccks\ object, using the \tmu{pdfmvc.cks} constructor.

I will use four variables from the ``quakes'' data in the datasets package:

<<>>=
conditions = c (lat=-20, long=180)
cfh = pdfmvc.cks (quakes, conditions=conditions)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (cfh, xlim = c (0, 800) )
@
<<fig=TRUE, width=4.75, height=3>>=
plot (cfh, TRUE, xlim = c (0, 800) )
@
\end {center}

The model above, gives the bivariate density estimate of depth and magnitude, conditional on the near-mean values of latitude and longitude.

I found this interesting, because it suggests that, for a given location, there are two depth-based clusters of earthquakes.

So, here's some similar models, but using different locations:

<<>>=
cfh.AA = pdfmvc.cks (quakes, conditions = c (lat=-30, long=170) )
cfh.AB = pdfmvc.cks (quakes, conditions = c (lat=-30, long=180) )
cfh.BA = pdfmvc.cks (quakes, conditions = c (lat=-20, long=170) )
cfh.BB = cfh
@

\begin {center}
<<fig=TRUE, width=4.75, height=4.75>>=
plot_2x2 (cfh.AA, cfh.AB, cfh.BA, cfh.BB,
    "lat=-30, long=170", "lat=-30, long=180",
    "lat=-20, long=170", "lat=-20, long=180",
    xlim = c (0, 800) )
@
\end {center}

And what if we reverse the relationship (and model latitude and longitude conditional on depth), and ignore magnitude...

<<>>=
cfh.A = pdfmvc.cks (quakes [,-4], conditions = c (depth=100) )
cfh.B = pdfmvc.cks (quakes [,-4], conditions = c (depth=500) )
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (cfh.A, main="depth=100")
@
<<fig=TRUE, width=4.75, height=3>>=
plot (cfh.B, main="depth=500")
@
\end {center}

Likewise, we can construct a \cdfmvccks\ object, using the \tmu{cdfmvc.cks} constructor, however, I'm going to bypass the example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Categorical Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can construct a \pmfuvcat\ object, using the \tmu{pmfuv.cat} constructor.

I will use the state region variable from the datasets package: 

<<>>=
fh = pmfuv.cat (region)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (fh)
@
\end {center}

The resulting object is a function, which maps an integer or character vector to a numeric vector:
<<>>=
fh (1)
fh ("North Central")
@

Likewise, we can construct \cdfuvcat\ and \qfuvcat\ objects, using the \tmu{cdfuv.cat} and \tmu{qfuv.cat} constructors. However, as the categorical variable is not clearly ordinal, I will bypass the examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Categorical Distributions\\Conditional on A Continuous Variable}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It's possible to construct a \pmfccatcks\ object, using the \tmu{pmfc.cat.cks} constructor.

This gives a categorical distribution (currently, univariate PMF only) conditional on a continuous variable.

I will use the ``iris'' data from the datasets package:

<<>>=
mean.Sepal.Length = mean (iris.Sepal.Length)
fh = pmfc.cat.cks (iris.Species, iris.Sepal.Length, at=mean.Sepal.Length)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (fh)
@
\end {center}

Currently, \pmfccatcks\ models are derived by using Bayes Theorem with a categorical set, described later.

This in turn, can be used for classification purposes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Empirical-Like Distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can construct a \cdfel\ object, using the \tmu{cdf.el} constructor:

<<>>=
Fh = cdf.el (Height)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (Fh)
@
\end {center}

Likewise, we can construct a \qfel\ object, using the \tmu{qf.el} constructor:

<<>>=
Fh.inv = qf.el (Height)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (Fh.inv)
@
\end {center}

Unlike continuous kernel smoothing, empirical-like models don't ``Smooth'' the model, they're completely nonparametric. They compute a set of points, representing cumulative probabilities, and interpolate the points with a cubic Hermite spline. However, the resulting functions don't necessarily appear smooth.

\el\ models may be preferable to \cks\ models, if you want to compute the median or other quantiles, without any smoothing. These quantiles can be regarded as quantiles of the data, itself, rather than estimates derived from a model.

As with \qfuvcks\ models, \qfel\ models are not the exact inverse their corresponding CDF.

Empirical-like models require unique x values, and a small amount of random variation is automatically added if they're not unique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Distribution Sets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Here, a distribution set is a set of one or more probability distributions.

Currently, there are three types, and this may be changed significantly, in the future:
\begin{itemize}
	\item \textbf {Categorical Set}\\One univariate probability distribution for each (categorical) level, out of many (categorical) levels.
	\item \textbf {Conditional Set}\\Similar to a categorical set, except that there's one univariate-conditional probability distribution for each set of conditions, rather than each level.
	\item \textbf {Marginal Set}\\One univariate probability distribution for each variable, out of many variables.
\end{itemize}

We can construct categorical sets, using any of the univariate constructors, given so far.

Lets construct \pdfuvcks\ models of sepal length, grouped by species:

<<>>=
cs = categorical.set (pdfuv.cks, iris.Sepal.Length, group.by=iris.Species)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (cs)
@
\end {center}

Now, lets construct a conditional set, of the density of volume, conditional on different heights:
<<>>=
conditions = cbind (Height = c (20, 24, 28) )
cond.set = conditional.set (pdfc.cks, trees [,2:3], group.by=conditions)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (cond.set)
@
\end {center}

Note that it's possible that the bandwidth is too low, and increasing the bandwidth may minimize the bimodal effect, above.

We can construct marginal sets using any of the univariate constructors, given so far.

Lets construct marginal \qfel\ objects:

<<>>=
ms = marginal.set (qf.el, trees)
@

\begin {center}
<<fig=TRUE, width=4.75, height=6>>=
plot (ms)
@
\end {center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Multivariate Probabilities}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Here, multivariate probability refers to the probability of observing multiple random variables between pairs of lower and upper limits. In theory, such probabilities could be computed from the multivariate PMF or PDF, however (here at least), it's more efficient to compute them from the multivariate CDF.

Using the trees data, we can compute the probability that girth, height and volume are all between arbitrary pairs of values.

We can use the \tmu{probmv} function, which has three arguments, the multivariate CDF, a vector of lower limits and a vector of upper limits:
<<>>=
#multivariate cdf
Fh = cdfmv.cks (trees)
@

<<>>=
#approximate first and third quartiles
a = c (28, 22, 0.55)
b = c (38, 24, 1.05)
@

<<>>=
cbind (lower=a, upper=b)
@

<<>>=
#multivariate probability
probmv (Fh, a, b)
@

Note that it's possible to compute multiple regions at once by making a and b matrices with each row representing one region. Also note that currently, variables names are ignored, so they must be in the same order as the variables used to construct the CDF.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Chained Quantile Functions\\And Random Number Generation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In addition to the probability distributions presented earlier, it's also possible to construct what I refer to as chained quantile functions.

Standard quantile functions can be used to compute univariate random numbers via (standard) inversion sampling. And chained quantile functions (currently, for continuous kernel smoothing only) can be used to compute multivariate random numbers, via nonstandard inversion sampling.

Chained quantile functions work by:
\begin {enumerate}
\itemsep -0.15cm
	\item Fitting a standard quantile function, to the first variable's observations, $\mathbf {x}_{[,1]}^*$.
	\item Using that quantile function to map the first variable's (input) probabilities, $\mathbf {p}_{[,1]}$, to (output) quantiles $\mathbf {q}_{[,1]}$.
	\item (Assuming that there are two or more variables).\\
		Iterating over each evaluation point and each subsequent variable.\\
		For each (ith) evaluation point and for each (jth) subsequent variable:
	\begin {enumerate}
	\itemsep -0.15cm
		\item Fitting a conditional quantile function to a subset of variables, $\mathbf {x}_{[i, 1:j]}^*$, conditional on the previous variables, $\mathbf {q}_{[i, 1:(j - 1)]}$.
		\item And then using that conditional quantile function to evaluate the current point, for the next variable.
	\end {enumerate}
\end {enumerate}

The convenience function, \tmu{ph.rng}, takes two arguments, the univariate or chained quantile function, and the number of random numbers to generate, then evaluates the quantile function, using a vector or matrix of uniform random numbers.

<<>>=
chF.inv = chqf.cks (trees)
synthetic.data = ph.rng (chF.inv, 31)
@

<<>>=
#convenience function
plot.trees.data = function (x, main)
{   Height = x [,"Height"]
    Girth = x [,"Girth"]
    Volume = x [,"Volume"]
    scatterplot3d (Height, Girth, Volume,
        main=main, type="h", angle=112.5, pch=16)
}
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
#original data
plot.trees.data (trees, "original data")
@

<<fig=TRUE, width=4.75, height=3>>=
#synthetic data
plot.trees.data (synthetic.data, "synthetic data")
@
\end {center}

Note that I've used the same sample size, however, you could use any sample size.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Parameter Estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Both univariate and conditional probability distributions can be used to compute probabilities, parameter estimates and other parameter-like estimates. Parameter (and parameter-like) estimates, include the mean, median, mode, other quantiles, standard deviation, variance, skewness and kurtosis.

The mean, standard deviation, variance and higher moments can be computed from the PMF or continuous CDF. The mode can be computed from the PMF or PDF. Probabilities can be computed from the CDF, and the QF can be used for the median and other quantiles. In the future, I may allow automatic conversion between the PMF/PDF, CDF and QF, however, I note that such conversion may be less efficient.

I'm going to use a conditional distribution, and compute its mean, median and mode:

<<>>=
#conditional distributions
conditions = c (Height=24)
cfh = pdfc.cks (trees [,2:3], conditions=conditions)
cFh = cdfc.cks (trees [,2:3], conditions=conditions)
cFh.inv = qfc.cks (trees [,2:3], conditions=conditions)
@

<<>>=
#mean, median and mode
mean.Volume = ph.mean (cFh)
median.Volume = cFh.inv (0.5)
mode.Volume = ph.mode (cfh)
@

<<>>=
cbind (statistic = c ("mean", "median", "mode"),
    value = c (mean.Volume, median.Volume, mode.Volume) )
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (cfh)
abline (v = c (mean.Volume, mode.Volume) )
abline (v=median.Volume, lty=2)
@
\end {center}

<<>>=
#and just as an example, the variance, skewness and kurtosis...
ph.var (cFh)
ph.skewness (cFh)
ph.kurtosis (cFh)
@

Note that currently, standard deviation, variance and higher moments, should be regarded as unreliable (because the smoothing algorithm tends to inflate their values), however, they can still be used as an exploratory tool, especially for the purpose, of comparing different conditional probability distributions.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
My main sources are:

\ind \tmu {R's stats Package}

\ind \tmu {Wikipedia}

Also, this package uses the following R packages, or has been influenced by them:

\ind\tmu {intoo: Object Oriented Extensions}

\indf Spurdle, A.

\ind \tmu {barsurf: Bar, Surface and Related Plots}

\indf Spurdle, A.

\ind \tmu {bivariate: Bivariate Probability Distributions}

\indf Spurdle, A.

\ind \tmu {mvtnorm: Multivariate Normal and t Distributions}

\indf Genz, A., Bretz, F., Miwa, T., Mi, X. \& Hothorn, T.

\ind \tmu {KernSmooth: Functions for Kernel Smoothing Supporting Wand \& Jones (1995)}

\indf Wand, M. \& Ripley, B.

\ind \tmu {mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation}

\indf Wood, S.

\ind \tmu {fclust: Fuzzy Clustering}

\indf Giordani, P., Ferraro, M.B. \& Serafini, A.

\ind \tmu {kubik: Cubic Hermite Splines}

\indf Spurdle, A.

\ind \tmu {scatterplot3d: 3D Scatter Plot}

\indf Ligges, U., Maechler, M. \& Schnackenberg, S.

\ind \tmu {MASS: Support Functions and Datasets for Venables and Ripley's MASS}

\indf Ripley, B.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix A:\\Multivariate Probabilities}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can compute the probability that a single (continuous) random variable is between a pair of values as:
\begin {equation*}
\mathbb {P} (a \leq X \leq b)  = F_X (b) - F_X (a)
\end {equation*}

Where a is the lower limit and b is the upper limit.

This is the area under a univariate PDF.

Likewise, we can compute the probability that two (continuous) random variables are between two pairs of values as:

\begin {align*}
\mathbb {P} (a_1 \leq X_1 \leq b_1, \hspace {0.3cm} a_2 \leq X_2 \leq b_2)
	&= \sum P^{\binom{2}{2}} - \sum P^{\binom{2}{1}} + \sum P^{\binom{2}{1}}\\ \\
	&= F_{(X1, X2)} (b_1, b_2)\\
	&- [F_{(X1, X2)} (a_1, b_2) + F_{(X1, X2)} (b_1, a_2)]\\
	&+ F_{(X1, X2)} (a_1, a_2)
\end {align*}

Where $\sum P^{\binom{m}{k}}$ is shorthand for the sum of the m-variate CDF evaluated with each possible combination of k b-terms and (m - k) a-terms.

And where a is a vector of lower limits and b is a vector of upper limits.

This is the volume under the bivariate PDF.

For three and four variables we have:

\begin {align*}
&\mathbb {P} (a_1 \leq X_1 \leq b_1, \hspace {0.3cm} a_2 \leq X_2 \leq b_2, \hspace {0.3cm} a_3 \leq X_3 \leq b_3)\\
	&\hspace{1cm}= \sum P^{\binom{3}{3}} - \sum P^{\binom{3}{2}} + \sum P^{\binom{3}{1}} - \sum P^{\binom{3}{0}}\\ \\
	&\hspace{1cm}= F_{(X1, X2, X3)} (b_1, b_2, b_3)\\
	&\hspace{1cm}- [F_{(X1, X2, X3)} (a_1, b_2, b_3) + F_{(X1, X2, X3)} (b_1, a_2, b_3) + F_{(X1, X2, X3)} (b_1, b_2, a_3)]\\
	&\hspace{1cm}+ [F_{(X1, X2, X3)} (a_1, a_2, b_3) + F_{(X1, X2, X3)} (a_1, b_2, a_3) + F_{(X1, X2, X3)} (b_1, a_2, a_3)]\\
	&\hspace{1cm}- F_{(X1, X2, X3)} (a_1, a_2, a_3)\\ \\
&\mathbb {P} (a_1 \leq X_1 \leq b_1, \hspace {0.3cm} a_2 \leq X_2 \leq b_2, \hspace {0.3cm} a_3 \leq X_3 \leq b_3, \hspace {0.3cm} a_4 \leq X_4 \leq b_4)\\
	&\hspace{1cm}= \sum P^{\binom{4}{4}} - \sum P^{\binom{4}{3}} + \sum P^{\binom{4}{2}} - \sum P^{\binom{4}{1}} + \sum P^{\binom{4}{0}}
\end {align*}

More generally (given a continuous multivariate CDF, $F_{(X1, X2, ..., Xm)}$, for m random variables), we have:

\begin{equation*}
\mathbb {P} (a_1 \leq X_1 \leq b_1, \hspace {0.3cm} a_2 \leq X_2 \leq b_2, \hspace {0.15cm} ..., \hspace {0.15cm} a_m \leq X_m \leq b_m) =
\sum_{k \in [0, m] }\Big((-1)^{m-k} \sum P^{\binom{m}{k}}\Big)
\end{equation*}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix B:\\Conditional Formulae}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can compute univariate-conditional (continuous) distributions, with one random variable conditional on one other variable, using:
\begin {align*}
f_{Y} (y)
	&= f_{(Y \mid X = x)} (y)\\
	&= \frac {f_{(X1,X2)} (x_1=x, x_2=y)}{f_{X1} (x_1=x)}\\ \\
F_{Y} (y)
	&= F_{(Y \mid X = x)} (y)\\
	&= \int_{-\infty}^{y} \frac {f_{(X1,X2)} (x_1=x, x_2=u)}{f_{X1} (x_1=x)} du
\end {align*}

We can compute univariate-conditional (continuous) distributions, with one random variable conditional on multiple other variables, using:
\begin {align*}
f_{Y} (y)
	&= f_{(Y \mid X1=x1, X2=x2, ..., X[m-1] = x[m-1])} (y)\\
	&= \frac {f_{(X1,X2, ..., Xm)} (\$X, \$Y_{\text{uv}})}{f_{(X1, X2, ..., X[\text {ncon}])} (\$X)}\\
	&= \frac {f_{(X1,X2, ..., Xm)} (x_1=x_1, x_2=x_2, ..., x_{[\text {ncon}]}=x_{[\text {ncon}]}, \hspace {0.4cm} x_m=y)}{f_{(X1, X2, ..., X[\text {ncon}])} (x_1=x_1, x_2=x_2, ..., x_{[\text {ncon}]}=x_{[\text {ncon}]})}\\
	&= \frac {f_{(X1,X2, ..., Xm)} (x_1=x_1, x_2=x_2, ..., x_{[m-1]}=x_{[m-1]}, \hspace {0.4cm} x_m=y)}{f_{(X1, X2, ..., X[m-1])} (x_1=x_1, x_2=x_2, ..., x_{[m-1]}=x_{[m-1]})}\\ \\
F_{Y} (y)
	&= F_{(Y \mid X1=x1, X2=x2, ..., X[m-1]=x[m-1])} (y)\\
	&= \int_{-\infty}^{y} \frac {f_{(X1,X2, ..., Xm)} (\$X, \$U_{\text{uv}})}{f_{(X1,X2, ..., X_{[\text {ncon}]})} (\$X)} du\\
	&= \int_{-\infty}^{y} \frac {f_{(X1,X2, ..., Xm)} (x_1=x_1, x_2=x_2, ..., x_{[\text {ncon}]}=x_{[\text {ncon}]}, \hspace {0.4cm} x_m=u)}{f_{(X1,X2, ..., X[\text {ncon}])} (x_1=x_1, x_2=x_2, ..., x_{[\text {ncon}]}=x_{[\text {ncon}]})} du\\
	&= \int_{-\infty}^{y} \frac {f_{(X1,X2, ..., Xm)} (x_1=x_1, x_2=x_2, ..., x_{[m-1]}=x_{[m-1]}, \hspace {0.4cm} x_m=u)}{f_{(X1,X2, ..., X[m-1])} (x_1=x_1, x_2=x_2, ..., x_{[m-1]}=x_{[m-1]})} du
\end {align*}

Note that the convention in this package, is that conditioning variables are enumerated first.

In the univariate-conditional case, ncon (the number of conditions) is equal to m (the total number of variables) minus one.

This can be further generalized to compute multivariate-conditional (continuous) distributions, with M random variables conditional on multiple other variables, using:
\begin {align*}
	f_{Y1, Y2, ... YM} (y_1, y_2, ..., y_M)
	&= f_{(Y1, Y2, ..., YM \mid X1=x1, X2=x2, ..., X[\text {\text {ncon}}] = x[\text {\text {ncon}}])} (y_1, y_2, ..., y_M)\\
	&=\frac {f_{(X1,X2, ..., Xm)} (\$X, \$Y_{\text{mv}})}{f_{(X1, X2, ..., X[\text {\text {ncon}}])} (\$X)}\\ \\
	F_{Y1, Y2, ..., YM} (y_1, y_2, ..., y_M)
	&= F_{(Y1, Y2, ..., YM \mid X1=x1, X2=x2, ..., X[\text {ncon}]=x[\text {ncon}])} (y_1, y_2, ..., y_M)\\
	&= \int_{-\infty}^{y_1} \int_{-\infty}^{y_2} ... \int_{-\infty}^{y_M} \frac {f_{(X1,X2, ..., Xm)} (\$X, \$U_{\text {mv} })}{f_{(X1,X2, ..., X[\text {ncon}])} (\$X)} du_M, ..., du_2, du_1
\end {align*}

Where the subexpressions expand as follows:

$\$X: \{x_1=x_1, x_2=x_2, ..., x_{[\text {ncon}]}=x_{[\text {ncon}]}\}$

$\$Y_{\text{mv}}: \{x_{[\text {ncon}+1]}=y_1, x_{[\text {ncon}+2]}=y_2, ...,  x_{[\text {ncon}+M]}=y_M\}$

$\$U_{\text{mv}}: \{x_{[\text {ncon}+1]}=u_1, x_{[\text {ncon}+2]}=u_2, ...,  x_{[\text {ncon}+M]}=u_M\}$

Note that these formulae do not use all the data. A conditional window is computed, and observations outside the window are discarded. There needs to be at least one observation within the conditional window, otherwise, the denominator is undefined.

Note that it's not necessary to compute all of the expression, in each evaluation of the PDF or CDF. Rather, the denominator (which is a multivariate PDF) and the first part of the numerator (given later), can be computed when the object is constructed.

Also, note that it's not necessary to integrate the expression, as such. The algorithms for computing the multivariate PDFs and CDFs via kernel smoothing (also, given later), can be combined.

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix C (1):\\Discrete Kernel Smoothing Formulae}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Unstandardized discrete kernels, take the form:
\begin {align*}
k (x; \text {bw} ) &= ...\\
K (x; \text {bw} ) &= ...\\ \\
\text {hbw} &= \frac {\text {bw} - 1}{2}
\end {align*}

Where k and K are the kernel's PMF and CDF, respectively.\\
And where bw is the (odd positive) bandwidth parameter.

Unstandardized discrete kernels have zero mass outside the interval [-hbw, +hbw].

We can define additive-component distributions (or kernel mapping functions), as:
\begin {align*}
k^* (x; k, x_i^*, \text {bw}) &= k (x - x_i^*; \text {bw})\\
K^* (x; K, x_i^*, \text {bw}) &= K (x - x_i^*; \text {bw})
\end {align*}

Where $x_i^*$ is the (integer-valued) center of the of each additive-component distribution, and $x$ is the (integer-valued) point on the x-axis, where the function is evaluated.

Unbounded PMFs and CDFs can be computed, as follows:
\begin {align*}
\hat{f}_X (x; k, \text {bw}, n, \mathbf {x}^*, \mathbf {w}) &= \sum_i w_i k^* (x; k, x_i^*, \text {bw})\\
\hat{F}_X (x; K, \text {bw}, n, \mathbf {x}^*, \mathbf {w}) &= \sum_i w_i K^* (x; K, x_i^*, \text {bw})
\end {align*}

Where:
\begin {equation*}
w_i = \frac {h_i}{\sum_i h_i}
\end {equation*}

And where $\mathbf {x}^*$ is a vector of (integer-valued) bins and $\mathbf {h}$ is a vector of frequencies, both of which, are of length $n$, and $i \in [1, n]$.

In general, frequencies are integer-valued, however, there's no requirement for this.

Lower-bounded PMFs and CDFs can be computed by modifying the expressions above:
\begin {enumerate}
	\item Mass-estimates are truncated (i.e. equal to zero) below the lower limit.
	\item Remaining estimates are scaled based on the truncated area, such that the remaining mass-estimates sum to one.
	\item The x and h values are reflected about the lower limit, prior to smoothing, otherwise, mass-estimates near the lower limit tend to be too small.
\end {enumerate}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix C (2):\\Continuous Kernel Smoothing Formulae}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Standardized continuous kernels, take the form:
\begin {align*}
k (x) &= ...\\
K (x) &= ...
\end {align*}

Where k and K are the kernel's PDF and CDF, respectively.

Standardized continuous kernels have zero density outside the interval [-1, 1].

We can define additive-component distributions (or kernel mapping functions), using:
\begin {align*}
k^* (x; k, x_i^*, \text{bw}) &= \frac {2}{\text {bw} } k (\frac {2}{\text {bw} }(x - x_i^*) )\\
K^* (x; K, x_i^*, \text{bw}) &= K (\frac {2}{\text {bw} }(x - x_i^*) )
\end {align*}

Where bw is the bandwidth, $x_i^*$ is the center of the additive-component distributions, and $x$ is a point on the x-axis, where the function is evaluated.

Univariate PDFs and CDFs can be computed, as follows:
\begin {align*}
\hat {f}_X (x; k, \text{bw}, n, \mathbf{x}^*) &= \frac{\sum_i k^* (x; k, \text{bw}, x_i^*)}{n}\\
\hat {F}_X (x; K, \text{bw}, n, \mathbf{x}^*) &= \frac{\sum_i K^* (x; K, \text{bw}, x_i^*)}{n}
\end {align*}

Where $\mathbf{x}^*$ is a vector of length $n$, and $i \in [1, n]$.

Multivariate PDFs and CDFs can be computed, as follows:
\begin {align*}
\hat {f}_\mathbf{X} (\mathbf{x}; k, \mathbf{bw}, n, m, \mathbf{x}^*)
	&= \frac{\sum_i \Big(\$f_1 \times \$f_2 \times ... \times \$f_m \Big)}{n}\\
	&= \frac{\sum_i \Big( k^* (x_1; k, \text{bw}_1, x_{[i,1]}^*) \times k^* (x_2; k, \text{bw}_2, x_{[i,2]}^*) \times ... \times k^* (x_m; k, \text{bw}_m, x_{[i, m]}^*)\Big)}{n}\\ \\
\hat {F}_\mathbf{X} (\mathbf{x}; K, \mathbf{bw}, n, m, \mathbf{x}^*)
	&= \frac{\sum_i \Big(\$F_1 \times \$F_2 \times ... \times \$F_m \Big)}{n}\\
	&= \frac{\sum_i \Big( K^* (x_1; K, \text{bw}_1, x_{[i,1]}^*) \times K^* (x_2; K, \text{bw}_2, x_{[i,2]}^*) \times ... \times K^* (x_m; K, \text{bw}_m, x_{[i,m]^*})\Big)}{n}
\end {align*}

Where $\mathbf{bw}$ is a bandwidth vector, $\mathbf{x}^*$ is a matrix with $n$ rows (observations) and $m$ columns (variables), and $\mathbf{x}$ is a vector of points on the x-plane, where the function is evaluated.

The numerator of univariate-conditional PDFs and CDFs can be computed, as follows:
\begin {align*}
\hat {g}_Y (\mathbf{x}; k, \mathbf{bw}, n, m, \mathbf{x}^*)
	&= \frac{\sum_i \Big(((\$f_1)) \times ((\$f_2)) \times ... \times ((\$f_{(m-1)})) \times \$f_m \Big)}{n}\\
\hat {G}_Y (\mathbf{x}; k, K, \mathbf{bw}, n, m, \mathbf{x}^*)
	&= \frac{\sum_i \Big(((\$f_1)) \times ((\$f_2)) \times ... \times ((\$f_{(m-1)})) \times \$F_m \Big)}{n}
\end {align*}

Where, in general, the values of the subexpressions inside double brackets are precomputed.\\
(i.e. They're computed when objects are constructed, not when top-level functions are evaluated).

The numerator of multivariate-conditional PDFs and CDFs can be computed, as follows:
\begin {align*}
&\hat {g}_\mathbf{Y} (\mathbf{x}; M, \text{ncon}, k, \mathbf{bw}, n, \mathbf{x}^*)\\
	&\hspace{1cm} = \frac{\sum_i \Big(((\$f_1)) \times ((\$f_2)) \times ... \times ((\$f_\text{ncon})) \times \$f_{(\text{ncon} + 1)} \times \$f_{(\text{ncon}+ 2)} \times ... \$f_{(\text{ncon}+ M)} \Big)}{n}\\
&\hat {G}_\mathbf{Y} (\mathbf{x}; M, \text{ncon}, k, K, \mathbf{bw}, n, \mathbf{x}^*)\\
	&\hspace{1cm} = \frac{\sum_i \Big(((\$f_1)) \times ((\$f_2)) \times ... \times ((\$f_\text{ncon})) \times \$F_{(\text{ncon} + 1)} \times \$F_{(\text{ncon}+ 2)} \times ... \$F_{(\text{ncon}+ M)} \Big)}{n}
\end {align*}

Where $M$ is the number of random variables and ncon is the number of conditions.

Weighted versions of these formulae are created by substituting:
\begin {equation*}
\frac {\sum_i (\$ \text {SUB-EXPRESSION}) }{n}
\end {equation*}

With:
\begin {equation*}
\sum_i w_i (\$ \text {SUB-EXPRESSION})
\end {equation*}

Subject to:
\begin {equation*}
\sum_i w_i = 1
\end {equation*}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix D:\\Empirical-Like Formulae}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
An empirical cumulative distribution function, which is a step function, can be computed by:
\begin {equation*}
\mathbb {P} (X \leq x)  = \hat {F}_X (x; n, \mathbf {x}^*) = \frac {\sum_i I (x_i^* \leq x)}{n}
\end {equation*}

Where $I$ is an indicator function, which equals 1, if the enclosed logical expression is true, and equals 0, if false.

A proto-empirical-like distribution, which is also a step function, can be computed by modifying the formula above, to give:
\begin {equation*}
\mathbb {P} (X \leq x)  = \hat {G}_X (x; n, \mathbf {x}^*) = \frac {\big( \sum_i I (x_i^* \leq x) \big) - 1}{n - 1}
\end {equation*}

Expanding on an earlier point, this function can be used to generate a sequence of points:
\begin {equation*}
\{ \big (x_1^*, \hat {G}_X (x_1^*; n, \mathbf {x}^*) \big), \big (x_2^*, \hat {G}_X (x_2^*; n, \mathbf {x}^*) \big), ..., \big (x_n^*, \hat {G}_X (x_n^*; n, \mathbf {x}^*) \big)\}
\end {equation*}

An empirical-like distribution, which is a continuous function, can be computed by using a cubic Hermite spline to interpolate this sequence. 

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix E:\\Data Preparation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<>>=
data.prep (eval=FALSE, echo=TRUE)
@

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section* {Appendix F:\\Fuzzy Clustering\\(And Weighted Multivariate Kernel Smoothing)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Fuzzy clustering computes a membership matrix, from some data.

The values in the membership matrix represent the membership of each data point in each cluster, with each row representing one data point and each column representing one cluster.\\
(Note that row weights, not column weights, sum to one).

In some situations, is may be of interest to identify the clusters, only. In other situations, it may be of interest to identify the clusters, and model the properties one or more of those clusters.

It's possible to model each cluster using weighted kernel smoothing.

The following computes the membership matrix for three clusters:
<<>>=
membership = FKM.gk (unemployment, k=3, seed=2)$U
@

I'm going to extract the weights of the first cluster, and transform them, so that they sum to one:

<<>>=
w = membership [,1]
w = w / sum (w)
@

And a weighted model:

<<>>=
wfh.1 = pdfmv.cks (unemployment, w=w)
@

\begin {center}
<<fig=TRUE, width=4.75, height=3>>=
plot (wfh.1)
@

<<fig=TRUE, width=4.75, height=3>>=
k = 1 - w / max (w)
plot (unemployment, pch=16, col=rgb (k, k, k) )
@
\end {center}

And for the other two clusters:
<<>>=
w = membership [,2]
wfh.2 = pdfmv.cks (unemployment, w = w / sum (w) )
w = membership [,3]
wfh.3 = pdfmv.cks (unemployment, w = w / sum (w) )
@

All three:
\begin {center}
<<fig=TRUE, width=4.75, height=4.75>>=
plot_2x2 (wfh.1,, wfh.2, wfh.3, "cluster 1",, "cluster 2", "cluster 3")
@
\end {center}

\end{document}
